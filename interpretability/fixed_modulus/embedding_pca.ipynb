{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e503a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "## import stuff\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import utils.prng_data as prngs_data\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib import colors as mcolors\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "from utils.gpt2 import GPT, GPTConfig, MLP, CausalSelfAttention\n",
    "from utils.prng_data import lcg_vectorized, find_as, find_coprimes\n",
    "from utils.datasets import PRNGsDataset\n",
    "\n",
    "# usual imports\n",
    "import pickle as pl\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import copy\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "plt.rcParams.update({\"font.size\": 20})\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "dpi = 300\n",
    "cmap = 'coolwarm'\n",
    "\n",
    "# set the internal precision of float32 matrix multiplications: \n",
    "# https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html\n",
    "# “highest”, float32 matrix multiplications use the float32 datatype (24 mantissa bits with 23 bits explicitly stored) for internal computations.\n",
    "# “high”, float32 matrix multiplications either use the TensorFloat32 datatype (10 mantissa bits explicitly stored) or treat each float32 number as the sum of two bfloat16 numbers \n",
    "torch.set_float32_matmul_precision('high')\n",
    "dtype = torch.bfloat16 if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else torch.float32  # fp16 needs a further change of the code.\n",
    "\n",
    "## Toggle to true if you want to use GPU\n",
    "USE_GPU = False\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "elif USE_GPU and torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "else:\n",
    "    device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7984c82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description = 'Hyperparameters')\n",
    "    parser.add_argument('--main_seed', type = int, default = 1) # main seed for the experiments\n",
    "    ### Dataset hyperparams\n",
    "    parser.add_argument('--p_eval', type = int, default = 2048) # p for mod p\n",
    "    parser.add_argument('--num_as', type = int, default = 16) # number of as\n",
    "    parser.add_argument('--num_cs', type = int, default = 16) # number of cs\n",
    "    parser.add_argument('--num_examples_per_prng', type = int, default = 1) # number of examples\n",
    "    parser.add_argument('--total_examples', type = int, default = 1000_000) # number of examples\n",
    "    parser.add_argument('--context_len', type = int, default = 256) # number of examples\n",
    "    parser.add_argument('--chunk_size', type = int, default = 32) # number of examples\n",
    "    parser.add_argument('--period_min', type = int, default = 0) # min period of training\n",
    "    parser.add_argument('--period_max', type = int, default = 512) # max period of training\n",
    "    ### Model hyperparams\n",
    "    parser.add_argument('--n_layer', type = int, default = 1) # number of layers\n",
    "    parser.add_argument('--n_head', type = int, default = 1) # number of heads\n",
    "    parser.add_argument('--n_embd', type = int, default = 768)  # embedding dimension\n",
    "    parser.add_argument('--head_dim', type = int, default = 768) # number of heads\n",
    "    parser.add_argument('--act_name', type = str, default = 'relu') # activation\n",
    "    ### Optimization hyperparams\n",
    "    # parser.add_argument('--step', type = int, default = 2000) # number of training steps\n",
    "    parser.add_argument('--num_steps', type = int, default = 100_000) # number of training steps\n",
    "    parser.add_argument('--warmup_steps', type = int, default = 2048) # number of warmup steps\n",
    "    parser.add_argument('--lr_trgt', type = float, default = 3e-4) # the target learning rate\n",
    "    parser.add_argument('--lr_init', type = float, default = 1e-6) # initial learning rate\n",
    "    parser.add_argument('--lr_min', type = float, default = 1e-6) # final learning rate\n",
    "    parser.add_argument('--batch_size', type = int, default = 256) # batch size\n",
    "    # adamw hyperparams\n",
    "    parser.add_argument('--weight_decay', type = float, default = 1.0) # weight decay\n",
    "    parser.add_argument('--beta1', type = float, default = 0.9) # beta1 \n",
    "    parser.add_argument('--beta2', type = float, default = 0.99) # beta2\n",
    "    ### Evaluation hyperparams\n",
    "    parser.add_argument('--results_dir', type = str, default = './results')\n",
    "    parser.add_argument('--plots_dir', type = str, default = './plots')\n",
    "    # Other\n",
    "    parser.add_argument('--shifts', type = int, default = 0) # position of 1 to p_eval numbers in the sequence\n",
    "    \n",
    "    return parser.parse_args([\"--act_name=relu\", \"--context_len=256\", \"--batch_size=256\", \"--n_layer=1\", \"--p_eval=2048\", \"--total_examples=1000000\", \"--n_embd=768\", \\\n",
    "        \"--n_head=1\", \"--head_dim=768\", \"--warmup_steps=2048\", \"--num_steps=100000\", \"--num_examples_per_prng=1\", \"--lr_trgt=3e-04\", \"--weight_decay=1.0\"])\n",
    "\n",
    "config = parse_args()\n",
    "\n",
    "config.vocab_size = config.p_eval\n",
    "\n",
    "# # if I am not wrong, this seed only takes care of torch and not numpy\n",
    "# np.random.seed(config.main_seed)\n",
    "# torch.manual_seed(config.main_seed)\n",
    "# torch.cuda.manual_seed(config.main_seed)\n",
    "\n",
    "# Color\n",
    "N = (config.p_eval // 6) + 1  # number of colors to extract from each of the base_cmaps below\n",
    "base_cmaps = ['Greys', 'Purples', 'Reds', 'Oranges', 'Blues', 'Greens']\n",
    "\n",
    "n_base = len(base_cmaps)\n",
    "# we go from 0.2 to 0.8 below to avoid having several whites and blacks in the resulting cmaps\n",
    "colors = np.concatenate([plt.get_cmap(name)(np.linspace(0.2, 0.8, N)) for name in base_cmaps])\n",
    "custom_cmap = mcolors.ListedColormap(colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a67bcc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Useful functions\n",
    "\n",
    "def create_model(config):\n",
    "    gptconfig = GPTConfig(block_size=config.context_len, n_embd=config.n_embd, n_head=config.n_head, vocab_size=config.vocab_size, n_layer=config.n_layer, act_name=config.act_name)\n",
    "    model = GPT(gptconfig)\n",
    "    model.to(device)\n",
    "    return model\n",
    "\n",
    "\n",
    "def find_prng_parameters_test(config):\n",
    "    \"\"\" For a given p, find the possible values of a's and c's according to the Hull–Dobell Theorem: https://en.wikipedia.org/wiki/Linear_congruential_generator \"\"\"\n",
    "\n",
    "    a_list = prngs_data.find_as(config.p_eval)\n",
    "    c_list = prngs_data.find_coprimes(config.p_eval)\n",
    "\n",
    "    val_as = np.random.choice(a_list, min(64, len(a_list)))\n",
    "    val_cs = np.random.choice(c_list, min(64, len(c_list)))\n",
    "\n",
    "    ## use arbitary (a, c) as long as its not in val_a and val_c\n",
    "    # train_as = [i for i in range(1, config.p_eval) if i not in val_as]\n",
    "    # train_cs = [i for i in range(1, config.p_eval) if i not in val_cs]\n",
    "\n",
    "    return val_as, val_cs\n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "def lcg_vectorized_with_fixed_seed(p: int = 512, length: int = 8, a_list: list = [45], c_list: list = [123]) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Vectorized version of lcg function with fixed (0th) seed.\n",
    "    It supports multiple 'a' and 'c' values.\n",
    "    \"\"\"\n",
    "    ## Create mesh grid and flatten\n",
    "    a_mesh, c_mesh = np.meshgrid(a_list, c_list)\n",
    "    a_flat = torch.tensor(a_mesh.flatten(), dtype=torch.int64)\n",
    "    c_flat = torch.tensor(c_mesh.flatten(), dtype=torch.int64)\n",
    "\n",
    "    ## Generate initial seed(s)\n",
    "    initial_seeds = torch.arange(p, dtype=torch.int64)[:1]\n",
    "\n",
    "    def single_lcg(a, c, seed):\n",
    "        @torch.compile\n",
    "        def next_value(prev):\n",
    "            return (a*prev + c) % p\n",
    "        \n",
    "        sequence = [seed]\n",
    "        for _ in range(length - 1):\n",
    "            sequence.append(next_value(sequence[-1]))\n",
    "        \n",
    "        return torch.stack(sequence)\n",
    "\n",
    "    ## Vectorize over a, c, and initial seeds\n",
    "    results = torch.vmap(torch.vmap(single_lcg, in_dims=(None, None, 0)), in_dims=(0, 0, None), chunk_size=16)(a_flat, c_flat, initial_seeds)\n",
    "    \n",
    "    ## Reshape to combine all sequences\n",
    "    return results.reshape(a_flat.size(0), -1, length)\n",
    "\n",
    "\"\"\"PCA\"\"\"\n",
    "@torch.inference_mode()\n",
    "def pca_1d(embedding: torch.Tensor, components: tuple) -> torch.Tensor:\n",
    "    embedding = embedding.T  # (n_embd, vocab)\n",
    "    mean = embedding.mean(dim=1, keepdim=True)  # (n_embd, 1)\n",
    "    centered_data = embedding - mean\n",
    "\n",
    "    U, S, Vt = torch.linalg.svd(centered_data.T, full_matrices=False)\n",
    "    \n",
    "    # Select the specified components\n",
    "    selected_components = Vt[list(components), :]  # (len(components), n_embd)\n",
    "    \n",
    "    U, Vt = svd_flip(U, Vt)\n",
    "    \n",
    "    # Calculate and print the ratio of explained variance for the selected components\n",
    "    total_variance = torch.sum(S)\n",
    "    selected_variance = S[list(components)]\n",
    "    variance_ratio = (selected_variance / total_variance)\n",
    "    \n",
    "    print(f'Selected components {components} explain {variance_ratio} of the total variance')\n",
    "    \n",
    "    return (selected_components + mean.T)\n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "def pca_2d(embedding: torch.Tensor, components: tuple = (0, 1)) -> torch.Tensor:\n",
    "    embedding = embedding.T  # (n_embd, vocab)\n",
    "    mean = embedding.mean(dim=1, keepdim=True)  # (n_embd, 1)\n",
    "    centered_data = embedding - mean\n",
    "\n",
    "    U, S, Vt = torch.linalg.svd(centered_data.T, full_matrices=False)\n",
    "    \n",
    "    # Select the specified components\n",
    "    selected_components = Vt[list(components), :]  # (len(components), n_embd)\n",
    "    \n",
    "    U, Vt = svd_flip(U, Vt)\n",
    "    \n",
    "    # Calculate and print the ratio of explained variance for the selected components\n",
    "    total_variance = torch.sum(S)\n",
    "    selected_variance = torch.sum(S[list(components)])\n",
    "    variance_ratio = (selected_variance / total_variance).item()\n",
    "    \n",
    "    print(f'Selected components {components} explain {variance_ratio:.4f} of the total variance')\n",
    "    \n",
    "    return (selected_components + mean.T)\n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "def svd_flip(u, v):\n",
    "    # columns of u, rows of v\n",
    "    max_abs_cols = torch.argmax(torch.abs(u), 0)\n",
    "    idx = torch.arange(u.shape[1]).to(u.device)\n",
    "    signs = torch.sign(u[max_abs_cols, idx])\n",
    "    u *= signs\n",
    "    v *= signs.view(-1, 1)\n",
    "    return u, v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd764a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create model and load checkpoints\n",
    "model = create_model(config)\n",
    "\n",
    "ckpt_path = f'{config.results_dir}/chkpt_p{config.p_eval}_Tn{config.context_len}_N{config.total_examples}_ne{config.num_examples_per_prng}_n{config.n_embd}_h{config.n_head}_d{config.n_layer}_I{config.main_seed}_lr{config.lr_trgt:0.6f}_Tw{config.warmup_steps}_T{config.num_steps}_B{config.batch_size}_wd{config.weight_decay}.pth'\n",
    "ckpt = torch.load(ckpt_path, map_location=device)\n",
    "model.load_state_dict(ckpt['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "# plot_path = f'{config.plots_dir}/chkpt_p{config.p_eval}_Tn{config.context_len}_N{config.total_examples}_ne{config.num_examples_per_prng}_n{config.n_embd}_h{config.n_head}_d{config.n_layer}_I{config.main_seed}_lr{config.lr_trgt:0.6f}_Tw{config.warmup_steps}_T{config.num_steps}_B{config.batch_size}_wd{config.weight_decay}.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce568341",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fix random seeds\n",
    "np.random.seed(config.main_seed)\n",
    "torch.manual_seed(config.main_seed)\n",
    "torch.cuda.manual_seed(config.main_seed)\n",
    "\n",
    "## Generate test (a,c) using the Hull-Doebell theorem\n",
    "a_list, c_list = find_prng_parameters_test(config)\n",
    "a, c = a_list[1], c_list[1]\n",
    "\n",
    "## Generate the test dataset\n",
    "seq_collections = lcg_vectorized_with_fixed_seed(p = config.p_eval, length = config.context_len + 1, a_list = [a], c_list = [c])\n",
    "test_dataset = seq_collections[0, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b1afd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1d pca\n",
    "\n",
    "## Tuple of PCA components to be computed\n",
    "components = tuple([0])\n",
    "\n",
    "\n",
    "## Extract the embedding matrix, comupte 1d PCA, and project the embedding matrix onto the PCA\n",
    "wte = model.transformer.wte.weight.data.cpu()  # (vocab, n_embd)\n",
    "wte_pca = pca_1d(wte, components=components)\n",
    "results = wte @ wte_pca.t()\n",
    "\n",
    "\n",
    "## Plot PCA and its Fourier transform\n",
    "nrows = len(components)\n",
    "ncols = 2\n",
    "n_upto = 20  # We only plot a part of the PCA vector, for aesthetic reasons.\n",
    "        \n",
    "fig, axs = plt.subplots(figsize=(4*ncols, nrows*2), tight_layout=True)\n",
    "# plt.suptitle(\"PCA of Token Embeddings\")\n",
    "\n",
    "for i_c in range(len(components)):\n",
    "\n",
    "    plt.subplot(nrows, ncols, i_c * ncols + 1)\n",
    "    plt.grid(False)\n",
    "    plt.tick_params(axis='x', which='both', bottom=True, labelbottom=True)\n",
    "    plt.tick_params(axis='y', which='both', left=True, labelleft=True)\n",
    "    # plt.plot(results[:20, 0], '-.',label=f'{components[i] + 1}-th component')\n",
    "    plt.xticks(fontsize=15)\n",
    "    plt.yticks(fontsize=15)\n",
    "    plt.plot(results[:20, i_c], '.', color='purple')\n",
    "    plt.plot(results[:20, i_c], '-', alpha=0.2, color='purple')\n",
    "    plt.xlabel('number', fontsize=15)\n",
    "    plt.ylabel('value', fontsize=15)\n",
    "    # plt.legend(loc='upper right')\n",
    "\n",
    "    plt.subplot(nrows, ncols, i_c * ncols + 2)\n",
    "    ft = np.abs( np.fft.fft(results[:, :], axis=0) )\n",
    "    plt.grid(False)\n",
    "    plt.tick_params(axis='x', which='both', bottom=True, labelbottom=True)\n",
    "    plt.tick_params(axis='y', which='both', left=True, labelleft=True)\n",
    "    plt.xticks(fontsize=15, ticks=config.p_eval * np.array([0, 1/4, 1/2, 3/4, 1]), labels=[\"$0$\", \"$\\\\frac{\\\\pi}{2}$\", \"$\\\\pi$\", \"$\\\\frac{3\\\\pi}{2}$\", \"$2\\\\pi$\"])\n",
    "    plt.yticks(fontsize=15)\n",
    "    # plt.plot(ft, '-.',label=f'{components[0] + 1}-th component')\n",
    "    plt.plot(ft[:, i_c], '-', alpha=0.4, color='purple')\n",
    "    plt.scatter(ft[:, i_c].argmax(), ft[:, i_c].max(), s=20, color='purple')\n",
    "    # plt.annotate(f'{ft.argmax()}', (ft.argmax(), ft.max()), color='purple', fontsize=10)\n",
    "    # plt.xlabel(f'frequency ($\\\\times \\\\frac{{2\\\\pi}}{{ {config.p_eval} }}$)')\n",
    "    plt.xlabel('frequency', fontsize=15)\n",
    "    plt.ylabel('Fourier\\n amplitude', fontsize=15)\n",
    "    # plt.legend(loc='upper right')\n",
    "\n",
    "\n",
    "## Save the figure. Replace the filename and directory with your own.\n",
    "# plt.savefig(f\"{config.plots_dir}/pca_embedding_1d_p{config.p_eval}_d{config.n_layer}_h{config.n_head}_n{config.n_embd}.pdf\", dpi=400, format='pdf', bbox_inches=\"tight\")\n",
    "        \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb03378f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2d PCA\n",
    "\n",
    "## The two PCA components to be computed\n",
    "components = (1,2)\n",
    "\n",
    "\n",
    "## Extract the embedding matrix, and comupte 1d PCA\n",
    "wte = model.transformer.wte.weight.data.cpu()  # (vocab, n_embd)\n",
    "wte_pca = pca_2d(wte, components=components)\n",
    "    \n",
    "    \n",
    "## Make a Scatter plot of 2d PCA\n",
    "colors_2d = ['red', 'blue', 'green', 'purple']\n",
    "fig = plt.figure(figsize=(3.5, 3), constrained_layout=True)\n",
    "\n",
    "for number, embd_v in enumerate(wte[:]):\n",
    "    results = (embd_v @ wte_pca[0], embd_v @ wte_pca[1])\n",
    "    plt.scatter(results[0], results[1], alpha=0.2, color=colors_2d[number%4], s=10)\n",
    "    # plt.annotate(f'{number}', results, color=custom_cmap.colors[number], fontsize=5)\n",
    "\n",
    "plt.grid(False)\n",
    "plt.tick_params(axis='x', which='both', bottom=True, labelbottom=True, labelsize=15)\n",
    "plt.tick_params(axis='y', which='both', left=True, labelleft=True, labelsize=15)\n",
    "plt.xlabel(\"2nd PCA component\", fontsize=15)\n",
    "plt.ylabel(\"3rd PCA component\", fontsize=15)\n",
    "# plt.xlabel(f'PCA Component {components[0] + 1}')\n",
    "# plt.ylabel(f'PCA Component {components[1] + 1}')\n",
    "# plt.title(\"PCA of Token Embeddings\")\n",
    "\n",
    "\n",
    "## Save the figure. Replace the filename and directory with your own.\n",
    "# plt.savefig(f\"{config.plots_dir}/pca_embedding_2d_p{config.p_eval}_d{config.n_layer}_h{config.n_head}_n{config.n_embd}.pdf\", dpi=400, format='pdf', bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49915f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Autocorrelation embedding matrix\n",
    "\n",
    "wte = model.transformer.wte.weight.data.cpu()  # (vocab, n_embd)\n",
    "\n",
    "xlim = 700\n",
    "ylim = xlim\n",
    "xticks = np.arange(0, xlim, 256)\n",
    "yticks = xticks\n",
    "plt.figure(figsize=(5,4), tight_layout=True)\n",
    "plt.imshow( ((wte @ wte.T) / wte.norm(dim=-1)**2)[:xlim, :xlim], origin='lower', vmin=0, vmax=0.2)\n",
    "plt.xticks(ticks=xticks)\n",
    "plt.yticks(ticks=yticks)\n",
    "plt.tick_params(axis='x', which='both', bottom=True, labelbottom=True, labelsize=15)\n",
    "plt.tick_params(axis='y', which='both', left=True, labelleft=True, labelsize=15)\n",
    "plt.grid(False)\n",
    "cbar = plt.colorbar(fraction=0.05, aspect=18)\n",
    "cbar.ax.tick_params(labelsize=15)\n",
    "cbar.set_ticks([0.0, 0.05, 0.1, 0.15, 0.2])\n",
    "# cbar.set_ticks([0.0, 0.1, 0.2, 0.3, 0.4])\n",
    "# cbar.set_ticklabels(['0%', '20%', '40%', '60%', '80%', '100%'])\n",
    "\n",
    "\n",
    "## Save the figure. Replace the filename and directory with your own.\n",
    "# plt.savefig(f\"{config.plots_dir}/{config.act_name}/embedding_autocorrelation_p{config.p_eval}_d{config.n_layer}_h{config.n_head}_n{config.n_embd}.pdf\", dpi=400, format='pdf', bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
